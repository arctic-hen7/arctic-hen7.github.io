<!DOCTYPE html><html lang=en-US class=light><head><base href=https://arctic-hen7.github.io/><meta charset=UTF-8><meta name=viewport content="width=device-width, initial-scale=1.0"><link rel=stylesheet href=.perseus/static/tailwind.css><script type=module>window.__PERSEUS_RENDER_CFG = `{"tag/habits":"tag","posts":"posts","tag/programming":"tag","tag/economics":"tag","tag/css":"tag","tag/explanation":"tag","tag/overview":"tag","about":"about","":"","post/eede1692-be41-404a-93e8-d4b048db21bc":"post","post/bbb3e057-0f97-45a2-aa05-e791802fceee":"post","tag/open-source-admin":"tag","tag/productivity":"tag","tag/quickfix":"tag","tag/rust":"tag","tag/perseus":"tag","tag/design":"tag","tag/dev":"tag","contact":"contact","shortform":"shortform","post/03ab02d7-f196-46d7-a1f9-d0c951245889":"post","tag/tutorial":"tag","post/5633d380-fede-4c98-8c31-fc21610e4e5e":"post","tag/webdev":"tag","tag/frameworks":"tag","tag/ai":"tag","post/496eb33a-c137-4afa-a03c-2f165d522a98":"post","tag/linux":"tag","post/4726b90d-613e-4a45-b419-44cd6ec887a4":"post","tag/commentary":"tag","tag/self-development":"tag","post/c26d230f-a546-4da0-be18-9857e168d7b9":"post","post/7adca2f7-ab95-41e1-8d0d-4bbc020ca018":"post"}`;

        import init from "https://arctic-hen7.github.io/.perseus/bundle.js";
        async function main() {
            await init("https://arctic-hen7.github.io/.perseus/bundle.wasm");
        }
        main();

        
        

window.__PERSEUS_GLOBAL_STATE = `{"motd":"\\"[W]e should be able to do anything, but want to do only what is good\\" — Montaigne 1.26\\n","woke_up_on_time":true}`;</script><meta itemprop=__perseus_head_boundary><title>Welcome to the 'Layer 2' of AI | The Arctic Circle</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css><link rel=stylesheet href=.perseus/static/org.css><meta name=description content="We're entering a brave new world, but it's not why you think."><script>
window.__PERSEUS_INITIAL_STATE = `{"author":"Me","contents":"<p>\\nSince November 2022, and really since the release of Stable Diffusion several months prior, the world has been obsessed with generative AI, and I mean <i>obsessed</i>. In the mornings, when I read the news, I also read my GitHub <i>Explore</i> tab, which tells me about interesting new projects, and I&rsquo;ll read up on some of them, stashing some away to be reviewed later. Since the release of ChatGPT, 90% of the projects I have seen in that tab each day have been related directly to AI, if not to ChatGPT itself. This is certainly the phenomenon of hype (although a justified hype in some senses, given how phenomenally powerful the tech is), but it has given rise to an interesting new term: <i>prompt engineering</i>.\\n</p>\\n\\n<p>\\nIf you look up this strange term today, you&rsquo;ll find all manner of e-books on the subject, with their fair share of obscure-to-nonexistent publishers and spelling mistakes throughout, a telltale sign of people trying to capitalise on <i>the next big thing</i>. I won&rsquo;t bother explaining prompt engineering in detail: it&rsquo;s basically figuring out how to talk to AIs effectively, which applies especially for things like Stable Diffusion (ChatGPT, on the other hand, will pretty much figure out what you mean, for the most part).\\n</p>\\n\\n<p>\\nI want to take a different perspective on all this, though: my background is in framework development, and in engineering solutions that might best be called &rsquo;layer 1&rsquo;, to appropriate Ethereum&rsquo;s terminology: these are programs other people can use to build their own programs, and they range from web development frameworks that automate dozens of things that are normally quite hard (see <a href=\\"https://framesurge.sh/perseus/en-US\\">Perseus</a>) to decentralised platforms that aim to provide a foundation for distributed storage, computation, and smart contracts (see <a href=\\"https://quantorium.org/en-US\\">the Quantorium</a>). When a technology has stable and powerful layer 1 technology, this is usually when it starts to really take off, and I think this is exactly what we&rsquo;re seeing today in AI.\\n</p>\\n\\n<p>\\nFor the last several decades, AI development has been focused on getting those fundamentals right, with everyone starting largely from scratch in their own systems. Sure, we&rsquo;ve had basic architectures like convolutional neural networks (CNNS) for image classification and sequence-to-sequence transduction (for machine translation, among other use-cases), etc., but these aren&rsquo;t <i>frameworks</i>, they&rsquo;re just ideas. They&rsquo;re largely consigned to papers, and it was only with the development of framework-style systems like HuggingFace or TensorFlow that machine learning really became more open to a wider range of developers (i.e. those without maths degrees).\\n</p>\\n\\n<p>\\nToday, however, innovation has skyrocketed in AI because the programs that have been recently released (particualrly by OpenAI) are capable of being adapted to novel use-cases: if you ask ChatGPT to act as a translator, it will; if you ask it to play a chef, it will; if you ask it to explain quantum physics like you&rsquo;re five years old, it will. This kind of &rsquo;layer 2&rsquo; engineering, of changing the prompts we give to this system to make it do new things, is much easier than layer 1 engineering: all you need is a bit of common sense, an internet connection, and a bit of patience while you figure out the best way to do something. Contrast that to layer 1 engineering, where you&rsquo;re determining the mathematics of weight optimisation, and you&rsquo;ll understand why innovation in AI is suddenly taking off now, as opposed to five years ago.\\n</p>\\n\\n<p>\\nTo be clear, there have always been frameworks in AI that have helped developers do different things, but now we seem to have a base level of open-ish platforms (with more open alternatives being rapidly developed) that can be built on to provide novel applications very quickly: just the other day, I saw no fewer than three projects all using prompt engineering with ChatGPT to create applications that would allow one to have a conversation with a paper, or a book, or some other source of information, which could facilitate a whole new era of content summarisation. Again, this required very little &rsquo;hard&rsquo; innovation: they just had to write the right prompts and do a bit of JSON parsing.\\n</p>\\n\\n<p>\\nTo wrap up, I think we&rsquo;re now seeing a very interesting development in AI, but not because this generation of it is so much more powerful than the previous, but rather because it is so much more <i>open</i>, not in the sense of free and open-source, but in the sense of being easily expandable. To return to OpenAI&rsquo;s eventual vision of Artifical General Intelligence (AGI), the fundamental promise of AGI is to allow one to build applications that leverage AI trivially, in the same way that one can build a website with a no-code tool.\\n</p>\\n\\n<p>\\nAll this provides an interesting lens on the development of technology more broadly: this is the first major technological shift I have lived through as an adult, and what fascinates me, as a framework developer, is that, at the core of what drives the upward kink in a technology&rsquo;s adoption curve seems to be the ease of expansion: when a technology becomes a <i>framework</i> for future innovation. Perhaps this also provides an interesting level of hindsight into two projects: one you&rsquo;ve definitely heard of and one you probably never have: the World Wide Web, and Project Xanadu. The former was open and expandable, the latter tried to do all the innovation itself, and refused to be open. Today, <code>www</code> means one thing to almost everyone on the planet, while Xanadu conjures images of Coleridge&rsquo;s poetry, not the vast system of data transclusion its founders invisaged. So, from my perspective, if one lesson had to be taken away from all this for those who wish to build the technologies of the future, it would be this: <i>make them open</i>.\\n</p>","date":"2023-03-06","description":"We're entering a brave new world, but it's not why you think.","id":"496eb33a-c137-4afa-a03c-2f165d522a98","series":null,"tags":["ai","commentary","frameworks"],"title":"Welcome to the 'Layer 2' of AI","toc":"<div></div>"}`;
window.__PERSEUS_INITIAL_WIDGET_STATES = `{}`;
window.__PERSEUS_TRANSLATIONS = ``;</script><meta itemprop=__perseus_head_end><script>//</script><body class="bg-black text-white"><div id=root><header data-hk=1.11 class="w-full mb-20 backdrop-blur-lg {}"><div data-hk=1.12 class="sm:p-2 flex justify-between xl:justify-center items-center"><a data-hk=1.13 class="justify-self-start self-center m-3 ml-5 text-lg sm:text-xl text-bold title-font"href=/>The Arctic Site</a><div data-hk=1.14 class="xl:w-[38rem] 2xl:w-[55rem] 3xl:w-[70rem]"></div><div data-hk=1.15 class="md:hidden m-3 mr-5 tham tham-e-spin tham-w-6"><div data-hk=1.16 class=tham-box><div data-hk=1.17 class="tham-inner bg-white"></div></div></div><nav data-hk=1.18 class="hidden md:flex"><ul data-hk=1.19 class="mr-5 flex"><li data-hk=2.0 class="m-3 p-1"><a data-hk=2.1 href>Home</a><li data-hk=2.2 class="m-3 p-1"><a data-hk=2.3 href=about>About</a><li data-hk=2.4 class="m-3 p-1"><a data-hk=2.5 href=posts>The Arctic Circle</a><li data-hk=2.6 class="m-3 p-1"><a data-hk=2.7 href=shortform>The Ice Floes</a></ul></nav></div><nav data-hk=1.20 id=mobile_nav_menu class="sm:p-2 md:hidden w-full text-center justify-center hidden"><ul data-hk=1.21 class=mr-5><li data-hk=3.0 class="m-3 p-1"><a data-hk=3.1 href>Home</a><li data-hk=3.2 class="m-3 p-1"><a data-hk=3.3 href=about>About</a><li data-hk=3.4 class="m-3 p-1"><a data-hk=3.5 href=posts>The Arctic Circle</a><li data-hk=3.6 class="m-3 p-1"><a data-hk=3.7 href=shortform>The Ice Floes</a></ul></nav></header><main data-hk=1.22 class="mt-14 xs:mt-16 sm:mt-20 lg:mt-24"><div data-hk=1.0 class="flex flex-col lg:flex-row-reverse justify-center items-center w-full px-4 sm:px-8 lg:px-10"><h1 data-hk=1.1 class="text-4xl my-3 lg:hidden">Welcome to the 'Layer 2' of AI</h1><nav data-hk=1.2 class="styled-prose lg:sticky self-start top-14 xs:top-16 sm:top-20 lg:top-24 max-w-md max-h-[63vh] overflow-y-auto border-l border-neutral-600 pl-4 lg:mx-4 py-3"><div data-hk=1.3 class=max-w-sm><div></div></div></nav><div data-hk=1.4 class=min-w-0><h1 data-hk=1.5 class="text-4xl my-3 text-center hidden lg:block">Welcome to the 'Layer 2' of AI</h1><div data-hk=1.6 class="styled-prose lg:max-w-3xl xl:max-w-4xl 2xl:max-w-5xl min-w-0 min-h-0 mb-6"><p>Since November 2022, and really since the release of Stable Diffusion several months prior, the world has been obsessed with generative AI, and I mean <i>obsessed</i>. In the mornings, when I read the news, I also read my GitHub <i>Explore</i> tab, which tells me about interesting new projects, and I’ll read up on some of them, stashing some away to be reviewed later. Since the release of ChatGPT, 90% of the projects I have seen in that tab each day have been related directly to AI, if not to ChatGPT itself. This is certainly the phenomenon of hype (although a justified hype in some senses, given how phenomenally powerful the tech is), but it has given rise to an interesting new term: <i>prompt engineering</i>.<p>If you look up this strange term today, you’ll find all manner of e-books on the subject, with their fair share of obscure-to-nonexistent publishers and spelling mistakes throughout, a telltale sign of people trying to capitalise on <i>the next big thing</i>. I won’t bother explaining prompt engineering in detail: it’s basically figuring out how to talk to AIs effectively, which applies especially for things like Stable Diffusion (ChatGPT, on the other hand, will pretty much figure out what you mean, for the most part).<p>I want to take a different perspective on all this, though: my background is in framework development, and in engineering solutions that might best be called ’layer 1’, to appropriate Ethereum’s terminology: these are programs other people can use to build their own programs, and they range from web development frameworks that automate dozens of things that are normally quite hard (see <a href=https://framesurge.sh/perseus/en-US>Perseus</a>) to decentralised platforms that aim to provide a foundation for distributed storage, computation, and smart contracts (see <a href=https://quantorium.org/en-US>the Quantorium</a>). When a technology has stable and powerful layer 1 technology, this is usually when it starts to really take off, and I think this is exactly what we’re seeing today in AI.<p>For the last several decades, AI development has been focused on getting those fundamentals right, with everyone starting largely from scratch in their own systems. Sure, we’ve had basic architectures like convolutional neural networks (CNNS) for image classification and sequence-to-sequence transduction (for machine translation, among other use-cases), etc., but these aren’t <i>frameworks</i>, they’re just ideas. They’re largely consigned to papers, and it was only with the development of framework-style systems like HuggingFace or TensorFlow that machine learning really became more open to a wider range of developers (i.e. those without maths degrees).<p>Today, however, innovation has skyrocketed in AI because the programs that have been recently released (particualrly by OpenAI) are capable of being adapted to novel use-cases: if you ask ChatGPT to act as a translator, it will; if you ask it to play a chef, it will; if you ask it to explain quantum physics like you’re five years old, it will. This kind of ’layer 2’ engineering, of changing the prompts we give to this system to make it do new things, is much easier than layer 1 engineering: all you need is a bit of common sense, an internet connection, and a bit of patience while you figure out the best way to do something. Contrast that to layer 1 engineering, where you’re determining the mathematics of weight optimisation, and you’ll understand why innovation in AI is suddenly taking off now, as opposed to five years ago.<p>To be clear, there have always been frameworks in AI that have helped developers do different things, but now we seem to have a base level of open-ish platforms (with more open alternatives being rapidly developed) that can be built on to provide novel applications very quickly: just the other day, I saw no fewer than three projects all using prompt engineering with ChatGPT to create applications that would allow one to have a conversation with a paper, or a book, or some other source of information, which could facilitate a whole new era of content summarisation. Again, this required very little ’hard’ innovation: they just had to write the right prompts and do a bit of JSON parsing.<p>To wrap up, I think we’re now seeing a very interesting development in AI, but not because this generation of it is so much more powerful than the previous, but rather because it is so much more <i>open</i>, not in the sense of free and open-source, but in the sense of being easily expandable. To return to OpenAI’s eventual vision of Artifical General Intelligence (AGI), the fundamental promise of AGI is to allow one to build applications that leverage AI trivially, in the same way that one can build a website with a no-code tool.<p>All this provides an interesting lens on the development of technology more broadly: this is the first major technological shift I have lived through as an adult, and what fascinates me, as a framework developer, is that, at the core of what drives the upward kink in a technology’s adoption curve seems to be the ease of expansion: when a technology becomes a <i>framework</i> for future innovation. Perhaps this also provides an interesting level of hindsight into two projects: one you’ve definitely heard of and one you probably never have: the World Wide Web, and Project Xanadu. The former was open and expandable, the latter tried to do all the innovation itself, and refused to be open. Today, <code>www</code> means one thing to almost everyone on the planet, while Xanadu conjures images of Coleridge’s poetry, not the vast system of data transclusion its founders invisaged. So, from my perspective, if one lesson had to be taken away from all this for those who wish to build the technologies of the future, it would be this: <i>make them open</i>.</div></div></div><div data-hk=1.7 class="flex flex-col items-center w-full"><ul data-hk=1.8 class="flex justify-center text-center mt-4"><li data-hk=0.0 class=inline-block><a data-hk=0.1 class="block border-4 border-neutral-800 hover:bg-neutral-800 transition-colors duration-150 rounded-lg p-2 px-4 m-2 font-mono flex items-center"href=tag/ai>ai</a><li data-hk=0.2 class=inline-block><a data-hk=0.3 class="block border-4 border-neutral-800 hover:bg-neutral-800 transition-colors duration-150 rounded-lg p-2 px-4 m-2 font-mono flex items-center"href=tag/commentary>commentary</a><li data-hk=0.4 class=inline-block><a data-hk=0.5 class="block border-4 border-neutral-800 hover:bg-neutral-800 transition-colors duration-150 rounded-lg p-2 px-4 m-2 font-mono flex items-center"href=tag/frameworks>frameworks</a></ul><div data-hk=1.9 class="max-w-prose px-4 mt-8 giscus"></div></div><script data-hk=1.10 src=https://giscus.app/client.js data-repo=arctic-hen7/arctic-hen7.github.io data-repo-id=MDEwOlJlcG9zaXRvcnkzNjc4MTc5Njk= data-category=Comments data-category-id=DIC_kwDOFex08c4CSjuT data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=dark_high_contrast data-lang=en data-loading=lazy crossorigin async></script></main><footer data-hk=1.23 class="w-full flex justify-center py-5 bg-black"><p data-hk=1.24 class="mx-5 text-center">© arctic-hen7 2021-2022</footer></div><div id=__perseus_popup_error></div>